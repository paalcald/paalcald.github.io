#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:nil e:t email:nil f:t inline:t num:1
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+options: timestamp:t title:t toc:nil todo:t |:t
#+title: GCom: Practica 2
#+author: Pablo C. Alcalde
#+email: paalcald@ucm.es
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 28.1 (Org mode 9.6)
#+cite_export:
* Introducción
Se quiere agrupar los datos recibidos según estén mas o menos cerca dados los parámetros de la muestra, utilizaremos algoritmos de Clustering como KMeans ó DBSCAN con este fin.
* Material Usado
** Librerias y Paquetes
Para está practica se utilizarán a parte de las librerias que generalmente se utilizan en cualquier proyecto como /matplotlib/, /numpy/ y /scipy/ las siguientes librerias de python.
- Scikit-Learn
  Por su implementación de los diferentes algoritmos de clustering necesarios.
- Scipy
  Por su implementación de un método sencillo para dibujar diagramas de Voronoi.
** Comparación con los valores reales
Al tratar de realizar la comparación de los datos obtenidos con los /correctos/, nos dimos cuenta de que por la asignación arbitraría de etiquetas en las agrupaciones de la muestra, era necesarío algun algoritmo o criterío para identificar etiquetas de muestra con las etiquetas /correctas/.

Para resolver ese problema se reasignaron etiquetas \( \left\{ n_{1}, n_2, n_3, n_4 \right\} \) a la muestra y \( \left\{ m_{1}, m_2, m_3, m_4 \right\} \) al control, tal que \( \forall i, j, k, l \quad i \ne k \land j \ne l \implies n_{i} + m_{j} \neq n_k + m_l \) de este modo añadiremos los valores de las etiquetas asignadas para diferenciar donde el etiquetado coincidió de donde agrupamos incorrectamente, respecto al control, sin vernos influenciados por los cambios de nombre de las etiquetas en sí.
* Resultados
** Apartado 1
[[file:silhouette_coefs.png]]
** Apartado 2
[[file:dbscan.png]]
** Apartado 3
+ \( (0,0) \) pertenece al grado 1
+ \( (0, -1) \) pertenece al grado 0
* Conclusión
El número de clusters óptimo según el algoritmo ha resultado no corresponder al real suplementado en los datos, aún asi, un segundo resultado muy cercano a la optimalidad si coincide en el número de clusters y sólo nos equivocamos en ese caso en menos del 7% de los sujetos.
* Apéndice de Código
:PROPERTIES:
:UNNUMBERED: t
:END:
#+include: "./main.py" src python


